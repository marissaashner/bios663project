---
title: "Student Performance in Math Courses"
author: "Group 15 - Marissa Ashner, Ben Bulen, Kyle Grosser, Benjana Guraziu, Kushal Shah"
date: "4/29/2019"
output: html_document
---

<style type="text/css">

body{ /* Normal  */
      font-size: 12px;
  }
h1.title {
  font-size: 24px;
}
code.r{ /* Code block */
    font-size: 10px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 10px;
}
h1 { font-size: 22px; }

h2 { font-size: 20px; }

h3 { font-size: 18px; }

</style>

```{r, include = FALSE, warning = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)
library(tidyverse)
library(ggplot2)
library(GGally)
library(RColorBrewer)
```

```{r, warning = FALSE}
student <- read.table("student-mat.csv", sep=";", header=T)
student <- as_tibble(student)

student_all <- read.table("student-mat.csv", sep=";", header=T)
student_all <- as_tibble(student)
```

## Introduction

Portugal, in 2006, had an early school leaving rate of nearly 40% for 18-24 year olds, while the EU average was 15% (Eurostat 2007). Various analyses and interventions, therefore, have targeted school success in Portuguese adolescents and attempted to improve education overall. Portuguese students are lacking in mathematics performance, specifically.

We are therefore analyzing student data from two Portuguese secondary schools. This dataset measures mathematics course grade as the outcome, as a result of 30 predictors that range from demographic factors and family situation to student habits and extracurricular involvement. Specifically, data was collected from 395 students across an age range of 15-22 years (Cortez & Silva 2008).

Portuguese schools have a five-level grade classification system. With a final score measured from 0-20, categories are as follows: Excellent (16-20), Good (14-15), Satisfactory (12-13), Sufficient (10-11), Fail (0-9). For the purposes of this analysis, however, we analyzed exact score rather than score category.

In this analysis, we wish to predict student success in secondary school math as a result of two types of factors:

1. Unchangeable (family situation, parents' jobs, home location, etc.)
1. Targetable behavior (alcohol consumption, absences, extracurricular involvement, etc.)

The potential outcomes of this analysis are:

1. Identification of student behaviors that can be targeted through school-related programs or interventions in order to increase success
1. Indentification of inherent factors present in certain students' backgrounds that may put them at a disadvantage to begin with. Early identification without discrimination could result in these students being helped more effectively along the way.

## Data Description 

There are 30 inital covariates in the dataset. They are listed below and are described further in the Appendix.

Covariates: School, Sex, Age, Address, Famsize, Pstatus, Medu, Fedu, Mjob, Fjob, Reason, Guardian, Traveltime, Studytime, Failures, Schoolsup, Famsup, Paid, Activities, Nursery, Higher, Internet, Romantic, Famrel, Freetime, Goout, Dalc, Walc, Health, Absences

The response variables are: 

1. G1 - first period grade (numeric: from 0 to 20) 
2. G2 - second period grade (numeric: from 0 to 20) 
3. G3 - final grade (numeric: from 0 to 20, output target)

For this analysis, we will be considering the `G3` final grade as our response. The reason for this is that all three response variables are very highly correlated, which can be seen visually in the Appendix.

## Expoloratory Data Analysis and Variable Filtering

Write a paragraph about how it went here. Refer to plots in appendix. Maybe pick one or two plots to go here depending on room.

```{r}
### Distribution of the Response Variable 
outcomeplot1 <- student %>% 
  ggplot(aes(G3)) + geom_histogram(binwidth = 1, color='black', fill = 'grey') + 
  geom_vline(xintercept = mean(student$G3), lwd = 1, color = 'red') +
  labs(title = "Distribution of Final Math Grade", x="Final Grade", y="Count")
```

```{r, include = FALSE }
student %>% select(G1, G2, G3) %>% filter(G3 ==0 & G1 ==0 & G2 ==0)
```

Let's remove the students with a grade of 0, as that doesn't make much practical sense, especially since all of these students have grades higher than 0 first and/or second period.

```{r}
#**BB: agree with this approach, if we are feeling adventurous we could treat those 0's as missing and impute them but let's not over complicate things yet
# MICE code if we decide to impute ##
#student <- read.table("student-mat.csv", sep=";", header=T)
#student <- as.data.frame(student)

#student$G3[student$G3 == 0] <- NA
#md.pattern(as.data.frame(student)) #38 students missing G3, almost 10% of data is missing
#imputed_data <- mice(student, m=5, maxit = 50, method = "pmm", seed = 500)
#summary(imputed_data)
#imputed_data$imp$G3
#imputed<-complete(imputed_data)
```

```{r}
### Correlation for all continuous variables
#Note this throws a warning about variables being non-numeric but still produces output for all numeric arguments. Also gives additional reason to look at G3 only as G1/G2 are highly correlated with G3
corrplot1 <- ggcorr(student)
```

```{r}
student <- student %>% filter(G3 != 0) %>% select(-G1, -G2)
student_all <- student_all %>% select(-G1, -G2)
outcomeplot2 <- student %>% 
  ggplot(aes(G3)) + geom_histogram(binwidth = 1, color='black', fill = 'grey') + 
  geom_vline(xintercept = mean(student$G3), lwd = 1, color = 'red') +
  labs(title = "Distribution of Final Math Grade", x="Final Grade", y="Count")
```

```{r}
### Any difference in correlation after we drop G3=0 records?
#BB: I don't see any real differences
corrplot2 <- ggcorr(student)
```

```{r}
### Covariates and their relation to Final Grade
allvars <- student %>% 
  gather(key = "Variable", value = "Value", -G3) %>% 
  ggplot(aes(Value, G3)) + facet_wrap(~ Variable, scales = "free") + 
  geom_boxplot()
```

```{r}
## visualizing absences
abs1 <- student %>%
  # filter(absences < 23) %>%
  mutate(absences = factor(absences)) %>%
  ggplot(aes(absences, G3)) +
  geom_boxplot()

abs2 <- ggplot(student, aes(absences, G3)) +
  geom_jitter() +
  geom_smooth() +
  labs(title = "Final Grade by Absences", x = "Number of school absences", y = "Final grade")
```

Since the variable "absences" has a greater range of values than most of the other variables, and since after 22 absences the data becomes extremely sparse, we decided to group 23-93 absences within the value "23."

```{r}
## Pooling all absences greater than 22 into the 23 group
student <- student %>% 
  mutate(absences = case_when(
                      absences > 22 ~ 23L,
                      TRUE ~ .$absences))

## visualizing new absences variable
abs3 <- student %>%
  # filter(absences < 23) %>%
  mutate(absences = factor(absences)) %>%
  ggplot(aes(absences, G3)) +
  geom_boxplot()

abs4 <- ggplot(student, aes(absences, G3)) +
  geom_jitter() +
  geom_smooth()
```

"Absences" still has a much greater spread of values than the other variables, so we will try binning them to see how that effects our models. 

```{r}
## Binning absences into 5 groups with 5 absence numbers each (except for the last group that includes all absences greater than or equal to 20).

# Binning
student <- student %>% 
  mutate(absences = case_when(
                      absences < 5 ~ 1L, 
                      absences >= 5 & absences < 10 ~ 2L,
                      absences >= 10 & absences < 15 ~ 3L,
                      absences >= 15 & absences < 20 ~ 4L,
                      absences >= 20 ~ 5L,
                      TRUE ~ .$absences))

student_all <- student_all %>% 
  mutate(absences = case_when(
                      absences < 5 ~ 1L, 
                      absences >= 5 & absences < 10 ~ 2L,
                      absences >= 10 & absences < 15 ~ 3L,
                      absences >= 15 & absences < 20 ~ 4L,
                      absences >= 20 ~ 5L,
                      TRUE ~ .$absences))

## visualizing new absences variable
abs5 <- student %>%
  # filter(absences < 23) %>%
  mutate(absences = factor(absences)) %>%
  ggplot(aes(absences, G3)) +
  geom_boxplot()

abs6 <- ggplot(student, aes(absences, G3)) +
  geom_jitter() +
  geom_smooth()

abs7 <- student %>% 
  mutate(absences = factor(absences)) %>% 
  ggplot(aes(absences, G3, fill = absences)) +
  geom_violin(color = "gray") +
  geom_jitter(width = 0.2, alpha = 0.5) +
  labs(title = "Final Grade by Absences", x = "School Absences", y = "Final grade") +
  scale_fill_brewer(palette="Pastel2") +
  theme(legend.position="none") +
  scale_x_discrete(labels=c("<5", "5-9", "10-14", "15-19", ">20"))

```

```{r, fig.height = 3, fig.width = 6}
cowplot::plot_grid(abs2, abs7)
```

```{r}
## visualizing alcohol consumption

# weekend alcohol consumption (5 is highest)
wealcoholRDA <- student %>% 
  mutate(Walc = factor(Walc)) %>% 
  ggplot(aes(Walc, G3)) +
  geom_violin() +
  geom_jitter(width = 0.2, alpha = 0.5)


# workday alcohol consumption (5 is highest)
wdalcoholRDA <- student %>% 
  mutate(Dalc = factor(Dalc)) %>% 
  ggplot(aes(Dalc, G3, fill = Dalc)) +
  geom_violin(color = "gray") +
  geom_jitter(width = 0.2, alpha = 0.5) +
  labs(title = "Final Grade by Workday Alcohol Consumption", x = "Level of workday alcohol consumption (5 is very high)", y = "Final grade") +
  scale_fill_brewer(palette="Pastel2") +
  theme(legend.position="none")

## Time spent Studying
studyingRDA <- student %>% 
  mutate(studytime = factor(studytime)) %>% 
  ggplot(aes(studytime, G3, fill = studytime)) +
  geom_violin(color = "gray") +
  geom_jitter(width = 0.2, alpha = 0.5) +
  labs(title = "Final Grade by Time Spent Studying", x = "Weekly Study Time", y = "Final grade") +
  scale_fill_brewer(palette="Pastel2") +
  theme(legend.position="none") +
  scale_x_discrete(labels=c("<2 hours", "2-5 hours", "5-10 hours", ">10 hours"))


## Time spent travelling
travelRDA <- student %>% 
  mutate(traveltime = factor(traveltime)) %>% 
  ggplot(aes(traveltime, G3)) +
  geom_violin() +
  geom_jitter(width = 0.2, alpha = 0.5)

## Guardian
guardianRDA <- student %>% 
  mutate(guardian = factor(guardian)) %>% 
  ggplot(aes(guardian, G3)) +
  geom_violin() +
  geom_jitter(width = 0.2, alpha = 0.5)
```

Based on these plots, we decided to remove the variable "guardian" from our dataset since it didn't seem to have any influence on the outcome. 

We also observed that students in this dataset were from two different schools. Using school in our prediction of math course success, although it may increase prediction accuracy, would make our analysis less generalizable to the overall population of students. Since the overarching goal of this analysis is to identify factors that systematically affect secondary school student success in math courses, we decided to remove school as a predictor variable.

```{r}
student$age <- student$age - min(student$age)

student <- student %>% 
  select(-guardian) %>%
  select(-school)

student_all <- student_all %>% 
  select(-guardian) %>%
  select(-school)
```

### Collinearity Analysis 

Before running any models on our data, we want to check some computational diagnostics to see whether or not the covariates are collinear with each other or the intercept. To do that, we can first calculate the VIF and Tolerance values for each covariate. 
The covariates with the highest VIF values are shown below. Usually, ****

```{r}
covariates_int <- model.matrix(G3 ~ ., data = student )
covariates <- covariates_int %>% as.data.frame() %>% select(-`(Intercept)`)
student_d <- data.frame(G3 = student$G3, covariates)
model1 <- lm(G3 ~ ., data = student_d)
VIF = car::vif(model1)
Tol = 1/VIF
df = data.frame(Variable = names(VIF), VIF = VIF, Tolerance = Tol)
viftol <- df %>% arrange(desc(VIF)) %>% head(10) %>% knitr::kable(align = c("c", "c"))


#set up dataframe for all students dataset:
covariates_int_all <- model.matrix(G3 ~ ., data = student_all )
covariates_all <- covariates_int_all %>% as.data.frame() %>% select(-`(Intercept)`)
student_d_all <- data.frame(G3 = student_all$G3, covariates_all)
```

We then conduct an eigen-analysis of the correlation and scaled SSCP matrices to determine which covariates are most collinear with each other (via the correlation matrix) and with the intercept (via the scaled SSCP matrix). 

```{r}
xtx = t(covariates_int)%*%covariates_int
Ds_half <- diag(diag(xtx)^-0.5)
sscp <- Ds_half %*% xtx %*% Ds_half
eig_sscp <- eigen(sscp)$values
PCs_sscp <- prcomp(sscp)[2]
CI_sscp <- sqrt(eig_sscp[1]/eig_sscp)

cov_center <- apply(covariates, 2, function(y) y - mean(y))
C <- (t(cov_center)%*%cov_center)/dim(cov_center)[1]
Dc_half <- diag(diag(C)^-0.5)
R <- Dc_half %*% C %*% Dc_half
eig_corr <- eigen(R)$values
CI_corr <- sqrt(eig_corr[1]/eig_corr)
PCs_corr <- prcomp(R)[2]

df <- data.frame("Eigenvalue" = c(eig_corr), "Condition Index" = c(CI_corr))
df2 <- data.frame("Eigenvalue" = c(eig_sscp), "Condition Index" = c(CI_sscp))

df %>% tail(2) %>% knitr::kable(align = c("c", "c"))
df2 %>% filter(`Condition.Index` > 30) %>% knitr::kable(align = c("c", "c"))
```

From the Correlation Matrix, the highest Condition Index is still very small, which means no collinearity needs to be taken care of there. From the Scaled SSCP Matrix, there are two Condition Indices over 30.

```{r}
PCs_sscp$rotation[1:17,36:37]
```

Above shows the last 2 PC's of the Scaled SSCP Matrix for the 1st-17th covariates (the rest have values close to 0). We can see that the 8th and 11-17th covariates may be collinear with the intercept. These covariates are listed below.

```{r}
colnames(student_d)[c(7, 10:16)]
```

Many of these are factor variables that have reference values in the itnercept, so it makes sense they might be collinear. Therefore we decided not to remove any variables from our full model for collinearity issues.

### HILE Gauss Assumptions 

The homogeneity of variances, linearity, and gaussian errors asssumptions usually go hand in hand, and can be tested for each model using a residual analysis. For each model we run, we conduct an Anderson-Darling Test for Normality, and create residual plots. For all models in this report, all three of these diagnostics were met. The detailed tests and plots are in the Appendix. In terms of the existence assumption, we assume it holds since we have a finite sample. We also must assume the independence of observations assumption holds for our analysis, although we do not have enough information to fully support this claim. We are not given information on the students such as whether or not there are siblings or related students in the group, whether or not certain study groups were relevant, whether or not the classes were curved, or other qualities of the data that could result in a violation of this assumption. Therefore we move forward claiming all assumptions are satisified. 

## Models and Model Testing

There are many types of models to consider when building a regression classifier, such as Multiple Linear Regression, ANOVA, Linear Mixed Models, and Logistic/Poisson Regression. Classically, Linear Mixed Models are used when observations are related and Logistic/Poisson Regression are used for binrary outcomes. Since we are assuming our observations (the students) are independent and the outcome variable (final grade) is numeric, we narrow our model search down to Multiple Linear Regression and ANOVA. Since, at the beginning, we have many factor-type covariates, we begin exploring Multiple Linear Regression models and their fit to our data.

### Model 1: Full Model 

The first model we consider is a full model of all covariates possible and no interaction terms included. The model can be written as follows: 

$$y = \beta_0 + \beta_1X_1 + ... + \beta_{36}X_{36} + \epsilon$$

where $\beta_i$ represents ... and $X_i$ represents a covariate. There are more covariates than expected because each factor variable is recreated as a series of "dummy" variables. If a factor has $n$ levels, then there will be $n-1$ variables, and therefore parameters, corresponding to that covariate.

```{r}
#summary(model1)
```

```{r}
stud_resid = rstudent(model1)
dat_resid <- data.frame(RESID = stud_resid, ABS = abs(stud_resid), PRED = model1$fitted.values)
#dat_resid %>% arrange(-ABS) %>% filter(ABS > 2)
```

```{r}
nortest1 <- nortest::ad.test(dat_resid$RESID)
```

```{r, fig.height=4, fig.width=8}
p1 <- ggplot(data = dat_resid) + geom_histogram(aes(RESID), bins = 25) + labs(x = "Studentized Residuals")
p2 <- ggplot(data = dat_resid) + geom_point(aes(PRED, RESID)) + geom_hline(aes(yintercept = 0)) + labs(x = "Predicted Values", y = "Studentized Residuals")
diagnostics1 <- cowplot::plot_grid(p1, p2)
```

### Model 2: No Intercept

Since, from the collinearity analysis, several variables were significantly colinear with the intercept, we next try to build a model identical to the full model with the removal of the intercept. The model can be written as follows: 

$$y = \beta_1X_1 + ... + \beta_{36}X_{36} + \epsilon$$

```{r}
model2 <- lm(G3 ~ . -1, data = student_d)
#summary(model2)
```

```{r}
stud_resid = rstudent(model2)
dat_resid <- data.frame(RESID = stud_resid, ABS = abs(stud_resid), PRED = model2$fitted.values)
#dat_resid %>% arrange(-ABS) %>% filter(ABS > 2)
```

```{r}
nortest2 <- nortest::ad.test(dat_resid$RESID)
```

```{r, fig.height=4, fig.width=8}
p1 <- ggplot(data = dat_resid) + geom_histogram(aes(RESID), bins = 25) + labs(x = "Studentized Residuals")
p2 <- ggplot(data = dat_resid) + geom_point(aes(PRED, RESID)) + geom_hline(aes(yintercept = 0)) + labs(x = "Predicted Values", y = "Studentized Residuals")
diagnostics2 <- cowplot::plot_grid(p1, p2)
```

### Model 3: Stepwise Model

We then employed forward selection, backward selection, and combined forward and backward selection. Forward selection alone gives the full model, while backward selection and combined selection yield the same 13 covariates, excluding the intercept. We then adopted the result of backward selection as model 3.

```{r}
set.seed(12345)
modelback <- step(model1, direction = "backward", trace = F)
#modelback

modelboth <- step(model1, direction = "both", trace = F)
modelboth

modelforward <- step(model1, direction = "forward", trace = F)
#modelforward 
```

$$y = \beta_0 + \beta_1SEXM + \beta_2AGE + \beta_3ADDRESSU + \beta_4MJOBHEALTH + \\ \beta_5MJOBSERVICES + \beta_6FJOBTEACHER + \beta_7STUDYTIME + \beta_8FAILURES + \\ \beta_9SCHOOLSUPYES + \beta_{10}FAMSUPYES + \beta_{11}GOOUT + \beta_{12}HEALTH + \\ \beta_{13}ABSENCES + \epsilon$$

```{r}
#summary(modelback)
```

```{r}
stud_resid = rstudent(modelback)
dat_resid <- data.frame(RESID = stud_resid, ABS = abs(stud_resid), PRED = modelback$fitted.values)
#dat_resid %>% arrange(-ABS) %>% filter(ABS > 2)
```

```{r}
nortest3 <- nortest::ad.test(dat_resid$RESID)
```

```{r, fig.height=4, fig.width=8}
p1 <- ggplot(data = dat_resid) + geom_histogram(aes(RESID), bins = 25) + labs(x = "Studentized Residuals")
p2 <- ggplot(data = dat_resid) + geom_point(aes(PRED, RESID)) + geom_hline(aes(yintercept = 0)) + labs(x = "Predicted Values", y = "Studentized Residuals")
diagnostics3 <- cowplot::plot_grid(p1, p2)
```

### Model 4: Interaction Model

```{r, include = F}
# looking at all possible interaction terms of variables chosen in modelback
lm(G3 ~(sexM + Mjobhealth + Mjobservices + Fjobteacher + studytime + failures + 
          schoolsupyes + famsupyes + paidyes + internetyes + goout + health + absences)^2, 
   data = student_d) %>%
  summary()
```

Of all possible interaction pairs, several were significant. Among those, the most "interpretable" are studytime:schoolsupyes, studytime:famsupyes, studytime:goout, and schoolsupyes:goout. Interestingly, it appears that the effects of studytime are reduced for high values of goout; this may suggest that going out increases fatigue or distractedness, leading to a decreased benefit from studying. We add these significant interactions to the Stepwise Model to create our fourth model as defined below: 

$$y = \beta_0 + \beta_1SEXM + \beta_2AGE + \beta_3ADDRESSU + \beta_4MJOBHEALTH + \\ \beta_5MJOBSERVICES + \beta_6FJOBTEACHER + \beta_7STUDYTIME + \beta_8FAILURES + \\ \beta_9SCHOOLSUPYES + \beta_{10}FAMSUPYES + \beta_{11}GOOUT + \beta_{12}HEALTH + \\ \beta_{13}ABSENCES + \beta_{14}studytime:schoolsupyes + \beta_{15}studytime:famsupyes \\ + \beta_{16}studytime:goout + \beta_{17}schoolsupyes:goout + \epsilon$$

```{r}
model.int <- lm(G3 ~ sexM + Mjobhealth + Mjobservices + Fjobteacher + studytime + 
                  failures + schoolsupyes + famsupyes + paidyes + internetyes + goout + health +
                  absences + studytime:schoolsupyes + studytime:famsupyes + studytime:goout + schoolsupyes:goout, 
   data = student_d)
#summary(model.int)
```

```{r}
stud_resid = rstudent(model.int)
dat_resid <- data.frame(RESID = stud_resid, ABS = abs(stud_resid), PRED = model.int$fitted.values)
#dat_resid %>% arrange(-ABS) %>% filter(ABS > 2)
```

```{r}
nortest4 <- nortest::ad.test(dat_resid$RESID)
```

```{r, fig.height=4, fig.width=8}
p1 <- ggplot(data = dat_resid) + geom_histogram(aes(RESID), bins = 25) + labs(x = "Studentized Residuals")
p2 <- ggplot(data = dat_resid) + geom_point(aes(PRED, RESID)) + geom_hline(aes(yintercept = 0)) + labs(x = "Predicted Values", y = "Studentized Residuals")
diagnostics4 <- cowplot::plot_grid(p1, p2)
```

### Model 5: Reduced Stepwise Model

In order to build a more parsimonious model with less parameters, we took the approach of choosing a cut-off for the Type-III p-values from the added last tests of models 3 and 4 to determine which variables were most important in predicting final grade and were most useful to include in the model. For the fifth model, the Reduced Stepwise Model, we choose the six variables that had a type-III p-value of less than 0.0001 from the Stepwise Model 3. The model is outlined below:

$$y = \beta_0 + \beta_4MJOBHEALTH + \beta_5MJOBSERVICES + \beta_6FJOBTEACHER + \\ \beta_8FAILURES + \beta_9SCHOOLSUPYES + \beta_{11}GOOUT + \epsilon$$

```{r}
modelsmall <- lm(G3 ~ Mjobhealth + Mjobservices + Fjobteacher + failures + schoolsupyes + goout, data = student_d)
#summary(modelsmall)
```

```{r}
stud_resid_small = rstudent(modelsmall)
dat_resid_small <- data.frame(RESID = stud_resid_small, ABS = abs(stud_resid_small), PRED = modelsmall$fitted.values)
#dat_resid_small %>% arrange(-ABS) %>% filter(ABS > 2)
```

```{r}
nortest5 <- nortest::ad.test(dat_resid_small$RESID)
```

```{r, fig.height=4, fig.width=8}
p1.small <- ggplot(data = dat_resid_small) + geom_histogram(aes(RESID), bins = 25) + labs(x = "Studentized Residuals")
p2.small <- ggplot(data = dat_resid_small) + geom_point(aes(PRED, RESID)) + geom_hline(aes(yintercept = 0)) + labs(x = "Predicted Values", y = "Studentized Residuals")
diagnostics5 <- cowplot::plot_grid(p1.small, p2.small)
```

### Model 6: Reduced Interaction Model

For the fifth model, the Reduced Interaction Model, we choose the eleven variables that had a type-III p-value of less than 0.05 from the Interaction Model 4. If an interaction term was included, the base variables of that interaction term were also included, regardless of their significance. This model is outlined below:

$$y = \beta_0 + \beta_1MJOBHEALTH + \beta_2MJOBSERVICES + \beta_3FJOBTEACHER + \\ \beta_4STUDYTIME + \beta_5FAILURES + \beta_6SCHOOLSUPYES + \beta_7GOOUT + \\ \beta_8HEALTH + \beta_9ABSENCES + \beta_10studytime:schoolsupyes + \\ \beta_11studytime:goout + \epsilon$$ 

```{r}
smallint <- lm(G3 ~ Mjobhealth + Mjobservices + Fjobteacher + studytime + failures + schoolsupyes + absences + health + goout + studytime*schoolsupyes + studytime*goout, data = student_d)
#summary(smallint)
```

```{r}
stud_resid = rstudent(smallint)
dat_resid <- data.frame(RESID = stud_resid, ABS = abs(stud_resid), PRED = smallint$fitted.values)
#dat_resid %>% arrange(-ABS) %>% filter(ABS > 2)
```

```{r}
nortest6 <- nortest::ad.test(dat_resid$RESID)
```

```{r, fig.height=4, fig.width=8}
p1 <- ggplot(data = dat_resid) + geom_histogram(aes(RESID), bins = 25) + labs(x = "Studentized Residuals")
p2 <- ggplot(data = dat_resid) + geom_point(aes(PRED, RESID)) + geom_hline(aes(yintercept = 0)) + labs(x = "Predicted Values", y = "Studentized Residuals")
diagnostics6 <- cowplot::plot_grid(p1, p2)
```

## Model 6 All Subjects: Sensitivity analysis for dropping all subjects with G3=0

```{r}
model6_all <- lm(G3 ~ Mjobhealth + Mjobservices + Fjobteacher + studytime + failures + schoolsupyes + absences + health + goout + studytime*schoolsupyes + studytime*goout, data = student_d_all)
summary(model6_all)
```

## Extreme Residual Analysis  

Influence -- Cook's Distance
 - Values near 1 are bad 

Outlier -- Leverage Values 
 - Values greater than 2*p/n are bad

```{r}
p <- modelback$coefficients %>% length()
n <- nrow(student_d)
levcutoff <- 2*p/n
lev <- hatvalues(modelback)
cook <- cooks.distance(modelback)
```

take out values that are outlliers and run the regression again?

We decide to further remove a few outliers informally to increase the accuracy of our analysis. For example, we noticed that there were five students in the 20-22 age range. Since ages 15-18 had 70+ students per age, and age 19 had 19 students, we decided to remove the 20-22 year old students. For students of this age to be in secondary school, it is likely that they had certain extenuating circumstances that may not be represented in the dataset. Moreover, we simply have too few observations in this age range to be able to accurately analyze the effects of age on outcome in the 20+ region. Therefore, we went ahead and removed these students.

```{r}
table(student$age)

student <- student %>%
  filter(age < 20-15)
```


## Model Selection and Interpretation

Aside from looking at the Adjusted R2, can look at the AIC or SBC/BIC (smaller is better in both cases)

```{r}
library(tidyverse)

aic <- c(AIC(model1), AIC(model2), AIC(modelback), AIC(model.int), AIC(modelsmall), AIC(smallint))
bic <- c(BIC(model1), BIC(model2), BIC(modelback), BIC(model.int), BIC(modelsmall), BIC(smallint))
adjR <- c( summary(model1)$adj.r.sq,  summary(model2)$adj.r.sq,  summary(modelback)$adj.r.sq, summary(model.int)$adj.r.sq, summary(modelsmall)$adj.r.sq, summary(smallint)$adj.r.sq)
num.pred <- c(model1$coefficients %>% length, model2$coefficients %>% length, modelback$coefficients %>% length, model.int$coefficients %>% length, modelsmall$coefficients %>% length, smallint$coefficients %>% length)

selection <- data.frame(model = c("Model 1: Full Model", "Model 2: No Intercept", "Model 3: Stepwise Model", "Model 4: Stepwise Model + Interaction Terms", "Model 5: Reduced Stepwise Model", "Model 6: Reduced Interaction Model"), Parameters = num.pred, AICs = aic, BICs = bic, Adjusted.R.Squared = adjR)
selection %>% knitr::kable(align = c("c", "c"))
```

## Conclusion
Ultimately, we would select either our reduced step-wise model (Model 5) or our reduced interaction model (Model 6) because both of these models were able to explain a similar amount of variation and almost identical fits based on BIC. The main decision would lie in whether the investigator values a more parsimonious model (Model 5) or one with interaction terms (Model 6).

Regardless of the model, we were able able to find both static and modifiable variables that significantly impacted a student's final grade across several of our models. Certain parental career choices had a positive impact on final math grades, specifically, mother's in the health or service fields and fathers in a teaching field. This suggests that a parent's interests and/or work habits can have a positive influence on a student's academics. A static factor that negatively effected final grades was number of previous failures. This might be expected as students that have made poor grades in the past are likely to continue that trend without intervention. 

The modifiable factors that significantly impacted overall grade were time spent studying, time going out and number of absences. These are also variables that might be expected to be indicative of a student's academic performance. We also found these variables to effect final grade in an explainable direction. Increased time studying lended itself to higher grades, while more time out with friends and increased abscenses negatively impacted final grades. 

We would like to extend this dataset to span multiple years for future research. A longitudinal dataset with information on student dropout throughout secondary school would allow us to evaluate which factors significantly impact the dropout rate. This is seemingly the more important question for Portugal, based on the presentation of the dataset. 

The power of these models relies on their utility in the classroom. If teachers are able to gather and model student data like this early in a school year, then the teachers will be able to best help students achieve a more desireable grade. More attention can be given to student's with negatively influential static factors. Student's can also be motiviated to change modifiable behavior early in the year to make an overall positive impact on their grade.

## References 

[1] P. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th Future Business Technology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7.

[2] Eurostat, 2007. Early school-leavers. http://epp.eurostat.ec.europa.eu/.

## Appendix 

### Covariate Description 

1. SCHOOL - student's school (binary: "GP" - Gabriel Pereira or "MS" - Mousinho da Silveira)
2. SEX - student's sex (binary: "F" - female or "M" - male)
3. AGE - student's age (numeric: from 15 to 22)
4. ADDRESS - student's home address type (binary: "U" - urban or "R" - rural)
5. FAMSIZE - family size (binary: "LE3" - less or equal to 3 or "GT3" - greater than 3)
6. PSTATUS - parent's cohabitation status (binary: "T" - living together or "A" - apart)
7. MEDU - mother's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)
8. FEDU - father's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)
9. MJOB - mother's job (nominal: "teacher", "health" care related, civil "services" (e.g. administrative or police), "at_home" or "other")
10. FJOB - father's job (nominal: "teacher", "health" care related, civil "services" (e.g. administrative or police), "at_home" or "other")
11. REASON - reason to choose this school (nominal: close to "home", school "reputation", "course" preference or "other")
12. GUARDIAN - student's guardian (nominal: "mother", "father" or "other")
13. TRAVELTIME - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)
14. STUDYTIME - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)
15. FAILURES - number of past class failures (numeric: n if 1<=n<3, else 4)
16. SCHOOLSUP - extra educational support (binary: yes or no)
17. FAMSUP - family educational support (binary: yes or no)
18. PAID - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)
19. ACTIVITIES - extra-curricular activities (binary: yes or no)
20. NURSERY - attended nursery school (binary: yes or no)
21. HIGHER - wants to take higher education (binary: yes or no)
22. INTERNET - Internet access at home (binary: yes or no)
23. ROMANTIC - with a romantic relationship (binary: yes or no)
24. FAMREL - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)
25. FREETIME - free time after school (numeric: from 1 - very low to 5 - very high)
26. GOOUT - going out with friends (numeric: from 1 - very low to 5 - very high)
27. DALC - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)
28. WALC - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)
29. HEALTH - current health status (numeric: from 1 - very bad to 5 - very good)
30. ABSENCES - number of school absences (numeric: from 0 to 93)

### Exploratory Data Analysis 

```{r, fig.height = 3, fig.width = 6}
cowplot::plot_grid(corrplot1, corrplot2)
```

```{r, fig.height = 3, fig.width = 6}
cowplot::plot_grid(outcomeplot1, outcomeplot2)
```

```{r, fig.height = 6, fig.width = 9}
cowplot::plot_grid(wdalcoholRDA, wealcoholRDA, travelRDA, studyingRDA, guardianRDA, nrow = 2)
```

### Model Diagnostics 

```{r}
nortests <- data.frame(model = c(1:6), "Anderson-Darling p-value" = c(nortest1$p.value, nortest2$p.value, nortest3$p.value, nortest4$p.value, nortest5$p.value, nortest6$p.value))
nortests %>% knitr::kable(align = c("c", "c"))
```

```{r, fig.height = 18, fig.width = 6}
cowplot::plot_grid(diagnostics1, diagnostics2, diagnostics3, diagnostics4, diagnostics5, diagnostics6, nrow = 6)
```


