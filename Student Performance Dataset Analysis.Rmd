---
title: "Student Performance Dataset Analysis"
author: "Group 14- Marissa Ashner, Ben Bulen, Kyle Grosser, Benjana Guraziu, Kushal Shah"
date: "4/24/2019"
output: html_document
---

<style type="text/css">

body{ /* Normal  */
      font-size: 12px;
  }
h1.title {
  font-size: 24px;
}
code.r{ /* Code block */
    font-size: 10px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 10px;
}
</style>

```{r, include = FALSE, warning = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(tidyverse)
```

```{r, warning = FALSE}
student <- read.table("student-mat.csv", sep=";", header=T)
student <- as_tibble(student)
```

## Introduction 

## Data Description 

The inital covariates in the dataset are:

1. SCHOOL - student's school (binary: "GP" - Gabriel Pereira or "MS" - Mousinho da Silveira)
2. SEX - student's sex (binary: "F" - female or "M" - male)
3. AGE - student's age (numeric: from 15 to 22)
4. ADDRESS - student's home address type (binary: "U" - urban or "R" - rural)
5. FAMSIZE - family size (binary: "LE3" - less or equal to 3 or "GT3" - greater than 3)
6. PSTATUS - parent's cohabitation status (binary: "T" - living together or "A" - apart)
7. MEDU - mother's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)
8. FEDU - father's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)
9. MJOB - mother's job (nominal: "teacher", "health" care related, civil "services" (e.g. administrative or police), "at_home" or "other")
10. FJOB - father's job (nominal: "teacher", "health" care related, civil "services" (e.g. administrative or police), "at_home" or "other")
11. REASON - reason to choose this school (nominal: close to "home", school "reputation", "course" preference or "other")
12. GUARDIAN - student's guardian (nominal: "mother", "father" or "other")
13. TRAVELTIME - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)
14. STUDYTIME - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)
15. FAILURES - number of past class failures (numeric: n if 1<=n<3, else 4)
16. SCHOOLSUP - extra educational support (binary: yes or no)
17. FAMSUP - family educational support (binary: yes or no)
18. PAID - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)
19. ACTIVITIES - extra-curricular activities (binary: yes or no)
20. NURSERY - attended nursery school (binary: yes or no)
21. HIGHER - wants to take higher education (binary: yes or no)
22. INTERNET - Internet access at home (binary: yes or no)
23. ROMANTIC - with a romantic relationship (binary: yes or no)
24. FAMREL - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)
25. FREETIME - free time after school (numeric: from 1 - very low to 5 - very high)
26. GOOUT - going out with friends (numeric: from 1 - very low to 5 - very high)
27. DALC - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)
28. WALC - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)
29. HEALTH - current health status (numeric: from 1 - very bad to 5 - very good)
30. ABSENCES - number of school absences (numeric: from 0 to 93)

And the response variables are: 

1. G1 - first period grade (numeric: from 0 to 20) 
2. G2 - second period grade (numeric: from 0 to 20) 
3. G3 - final grade (numeric: from 0 to 20, output target)

For this analysis, we will be considering the `G3` final grade as our response. 

## Expoloratory Data Analysis and Variable Filtering

### Distribution of the Response Variable 

```{r}
student %>% 
  ggplot(aes(G3)) + geom_histogram(binwidth = 1, color='black', fill = 'grey') + 
  geom_vline(xintercept = mean(student$G3), lwd = 1, color = 'red') +
  labs(title = "Distribution of Final Math Grade", x="Final Grade", y="Count")
```

```{r}
student %>% select(G1, G2, G3) %>% filter(G3 ==0 & G1 ==0 & G2 ==0)
```

Let's remove the students with a grade of 0, as that doesn't make much practical sense, especially since all of these students have grades higher than 0 first and/or second period.

**BB: agree with this approach, if we are feeling adventurous we could treat those 0's as missing and impute them but let's not over complicate things yet
# MICE code if we decide to impute ##
student <- read.table("student-mat.csv", sep=";", header=T)
student <- as.data.frame(student)

student$G3[student$G3 == 0] <- NA
md.pattern(as.data.frame(student)) #38 students missing G3, almost 10% of data is missing
imputed_data <- mice(student, m=5, maxit = 50, method = "pmm", seed = 500)
summary(imputed_data)
imputed_data$imp$G3
imputed<-complete(imputed_data)


```{r}
student <- student %>% filter(G3 != 0) %>% select(-G1, -G2)
student %>% 
  ggplot(aes(G3)) + geom_histogram(binwidth = 1, color='black', fill = 'grey') + 
  geom_vline(xintercept = mean(student$G3), lwd = 1, color = 'red') +
  labs(title = "Distribution of Final Math Grade", x="Final Grade", y="Count")
```

### Covariates and their relation to Final Grade

```{r}
student %>% 
  gather(key = "Variable", value = "Value", -G3) %>% 
  ggplot(aes(Value, G3)) + facet_wrap(~ Variable, scales = "free") + 
  geom_boxplot()
```

```{r}
## visualizing absences
student %>%
  # filter(absences < 23) %>%
  mutate(absences = factor(absences)) %>%
  ggplot(aes(absences, G3)) +
  geom_boxplot()

ggplot(student, aes(absences, G3)) +
  geom_jitter() +
  geom_smooth()
```

Since the variable "absences" has a greater range of values than most of the other variables, and since after 22 absences the data becomes extremely sparse, we decided to group 23-93 absences within the value "23."

```{r}
## Pooling all absences greater than 22 into the 23 group
student <- student %>% 
  mutate(absences = case_when(
                      absences > 22 ~ 23L,
                      TRUE ~ .$absences))

## visualizing new absences variable
student %>%
  # filter(absences < 23) %>%
  mutate(absences = factor(absences)) %>%
  ggplot(aes(absences, G3)) +
  geom_boxplot()

ggplot(student, aes(absences, G3)) +
  geom_jitter() +
  geom_smooth()

```

"Absences" still has a much greater spread of values than the other variables, so we will try binning them to see how that effects our models. 

```{r}
## Binning absences into 5 groups with 5 absence numbers each (except for the last group that includes all absences greater than or equal to 20).

# Binning
student <- student %>% 
  mutate(absences = case_when(
                      absences < 5 ~ 1L, 
                      absences >= 5 & absences < 10 ~ 2L,
                      absences >= 10 & absences < 15 ~ 3L,
                      absences >= 15 & absences < 20 ~ 4L,
                      absences >= 20 ~ 5L,
                      TRUE ~ .$absences))

## visualizing new absences variable
student %>%
  # filter(absences < 23) %>%
  mutate(absences = factor(absences)) %>%
  ggplot(aes(absences, G3)) +
  geom_boxplot()

ggplot(student, aes(absences, G3)) +
  geom_jitter() +
  geom_smooth()

```



```{r}
## visualizing alcohol consumption

# weekend alcohol consumption (5 is highest)
student %>% 
  mutate(Walc = factor(Walc)) %>% 
  ggplot(aes(Walc, G3)) +
  geom_violin() +
  geom_jitter(width = 0.2, alpha = 0.5)

# workday alcohol consumption (5 is highest)
student %>% 
  mutate(Dalc = factor(Dalc)) %>% 
  ggplot(aes(Dalc, G3)) +
  geom_violin() +
  geom_jitter(width = 0.2, alpha = 0.5)

## Time spend Studying
student %>% 
  mutate(studytime = factor(studytime)) %>% 
  ggplot(aes(studytime, G3)) +
  geom_violin() +
  geom_jitter(width = 0.2, alpha = 0.5)

## Time spend travelling
student %>% 
  mutate(traveltime = factor(traveltime)) %>% 
  ggplot(aes(traveltime, G3)) +
  geom_violin() +
  geom_jitter(width = 0.2, alpha = 0.5)

## Guardian
student %>% 
  mutate(guardian = factor(guardian)) %>% 
  ggplot(aes(guardian, G3)) +
  geom_violin() +
  geom_jitter(width = 0.2, alpha = 0.5)
```

Based on these plots, we decided to remove the variable "guardian" from our dataset since it didn't seem to have any influence on the outcome. 

```{r}
student <- student %>% 
  select(-guardian)
```

### Initial BIC/R^2
Just to get an initial sense of influential variables we can look at all variables up to this point and examine fit statistics. This bit of code will look at permutations of all possible models up to nvmax and then report the best model based on fit criteria. For example, with nvmax of 30 it will produce the best model for 1 covariate, 2 covariates...up to the full model. 
```{r}
#Based on results the best models fall within 5-20 vars, so I only considered 20 variables in this run
best_subset <- regsubsets(G3 ~ ., student, nvmax = 20, method='exhaustive')
results<-summary(best_subset)

tibble(predictors = 1:20,
       adj_R2 = results$adjr2,
       Cp = results$cp,
       BIC = results$bic) %>%
  gather(statistic, value, -predictors) %>%
  ggplot(aes(predictors, value, color = statistic)) +
  geom_line(show.legend = F) +
  geom_point(show.legend = F) +
  facet_wrap(~ statistic, scales = "free")

#which.max(results$adjr2)
# 17 variables
#which.min(results$bic)
# 8 variables
#which.min(results$cp)
# 14 variables
```
### Looking at coefficients from best subset above based on BIC
# Notice based on BIC while the "best" option has 8 covariates, we really are not giving up much in the way of BIC using 7 covariates. We have presented both sets of coefficients here and generally want the more parsimonious model when all other criteria are the same.
```{r}
coef(best_subset, 8)
coef(best_subset, 7)
# We lose health when we drop to 7 variables. Otherwise all variables remain the same and are very similar in their effect size.
```


### Collinearity Analysis 

VIF and Tolerance 

```{r}
covariates_int <- model.matrix(G3 ~ ., data = student )
covariates <- covariates_int %>% as.data.frame() %>% select(-`(Intercept)`)
student_d <- data.frame(G3 = student$G3, covariates)
model1 <- lm(G3 ~ ., data = student_d)
VIF = car::vif(model1)
Tol = 1/VIF
df = data.frame(Variable = names(VIF), VIF = VIF, Tolerance = Tol)
df %>% arrange(desc(VIF)) %>% head(10) %>% knitr::kable(align = c("c", "c"))
```

Displayed above are the covariates with the highest Variance Inflation Factors.

EigenAnalysis for the Correlation and Scaled SSCP Matrices 

```{r}
xtx = t(covariates_int)%*%covariates_int
Ds_half <- diag(diag(xtx)^-0.5)
sscp <- Ds_half %*% xtx %*% Ds_half
eig_sscp <- eigen(sscp)$values
PCs_sscp <- prcomp(sscp)[2]
CI_sscp <- sqrt(eig_sscp[1]/eig_sscp)

cov_center <- apply(covariates, 2, function(y) y - mean(y))
C <- (t(cov_center)%*%cov_center)/dim(cov_center)[1]
Dc_half <- diag(diag(C)^-0.5)
R <- Dc_half %*% C %*% Dc_half
eig_corr <- eigen(R)$values
CI_corr <- sqrt(eig_corr[1]/eig_corr)
PCs_corr <- prcomp(R)[2]

df <- data.frame("Eigenvalue" = c(eig_corr), "Condition Index" = c(CI_corr))
df2 <- data.frame("Eigenvalue" = c(eig_sscp), "Condition Index" = c(CI_sscp))

df %>% tail(2) %>% knitr::kable(align = c("c", "c"))
df2 %>% filter(`Condition.Index` > 30) %>% knitr::kable(align = c("c", "c"))
```

From the Correlation Matrix, the highest Condition Index is still very small, which means no collinearity needs to be taken care of there. From the Scaled SSCP Matrix, there are two Condition Indices over 30.

```{r}
PCs_sscp$rotation[1:17,37:38]
```

Above shows the last 2 PC's of the Scaled SSCP Matrix for the 1st-17th covariates (the rest have values close to 0). We can see that the 8th and 11-17th covariates may be collinear with the intercept. These covariates are listed below.

```{r}
colnames(student_d)[c(8, 11:17)]
```

Many of these are factor variables that have reference values in the itnercept, so it makes sense they migh tbe collinear. Not going to remove any.

### Covariates of Potential Interest

## Models and Model Testing

There are many types of models to consider when building a regression classifier, such as Multiple Linear Regression, ANOVA, Linear Mixed Models, and Logistic/Poisson Regression. Classically, Linear Mixed Models are used when observations are related and Logistic/Poisson Regression are used for binrary outcomes. Since we are assuming our observations (the students) are independent for our purpose, and the outcome variable (final grade) is numeric, we narrow our model search down to Multiple Linear Regression and ANOVA. Since, at least to start, we have many factor-type covariates, we begin exploring Multiple Linear Regression models and their fit to our data.

### Model 1: Full Model 
*BB: since categorical vars are being grouped into several vars we have more than 30 Betas. I count 39 below.

The first model we consider is a full model of all covariates possible and no interaction terms included. The model can be written as follows: 

$$y = \beta_0 + \beta_1X_1 + ... + \beta_{37}X_{37} + \epsilon$$
where $\beta_i$ represents ... and $X_i$ represents a covariate. There are more covariates than expected because each factor variable is recreated as a series of "dummy" variables. If a factor has $n$ levels, then there will be $n-1$ variables, and therefore parameters, corresponding to that covariate.

```{r}
summary(model1)
```

** ADJUSTED R2 of 0.256 BOO ** 

#### Diagnostics

In order to determine the validity of this model, we must test the assumptions of linear regression. These assumptions include homogeneity of variances and gaussian errors. 

```{r}
stud_resid = rstudent(model1)
dat_resid <- data.frame(RESID = stud_resid, ABS = abs(stud_resid), PRED = model1$fitted.values)
dat_resid %>% arrange(-ABS) %>% filter(ABS > 2)
```

Since this set of largest studentized residuals are all greater than 2, it may be of interest to examine them in further detail. ** this part might be better in the outlier detection section ** 

###### Gaussian Errors Assumption 

```{r}
nortest::ad.test(dat_resid$RESID)
```

From the code above, since the p-value 0.4184 is greater than $\alpha = 0.05$, we accept the null hypothesis that the residuals are normally distributed. 

###### Homogeneity of Variances (and Gaussian Errors) Assumptions

```{r, fig.height=4, fig.width=8}
p1 <- ggplot(data = dat_resid) + geom_histogram(aes(RESID), bins = 25) + labs(x = "Studentized Residuals")
p2 <- ggplot(data = dat_resid) + geom_point(aes(PRED, RESID)) + geom_hline(aes(yintercept = 0)) + labs(x = "Predicted Values", y = "Studentized Residuals")
cowplot::plot_grid(p1, p2)
```

### Model 2: No Intercept

Since, from the collinearity analysis, several variables were significantly colinear with the intercept, we next try to build a model identical to the full model with the removal of the intercept. 

```{r}
model2 <- lm(G3 ~ . -1, data = student_d)
summary(model2)
```

** ADJUSTED R2 OF .9421 !!!!!! ** 

#### Diagnostics

```{r}
stud_resid = rstudent(model2)
dat_resid <- data.frame(RESID = stud_resid, ABS = abs(stud_resid), PRED = model2$fitted.values)
dat_resid %>% arrange(-ABS) %>% filter(ABS > 2)
```

```{r}
nortest::ad.test(dat_resid$RESID)
```

```{r, fig.height=4, fig.width=8}
p1 <- ggplot(data = dat_resid) + geom_histogram(aes(RESID), bins = 25) + labs(x = "Studentized Residuals")
p2 <- ggplot(data = dat_resid) + geom_point(aes(PRED, RESID)) + geom_hline(aes(yintercept = 0)) + labs(x = "Predicted Values", y = "Studentized Residuals")
cowplot::plot_grid(p1, p2)
```

### Stepwise Models 

```{r}
set.seed(12345)
modelback <- step(model1, direction = "backward", trace = F)
modelback

modelboth <- step(model1, direction = "both", trace = F)
modelboth

modelforward <- step(model1, direction = "forward", trace = F)
modelforward 
```

Backward and Both give the same variables.

Forward gives all the variables.

DOES NOT give same exact answer if we use model1 with intercept or model2 without to do the stepwise process. 

```{r}
summary(modelback)
```

This model so far gives us the best R2 adjusted and the most significant variables.

### Interaction Model (model.int)

```{r, include = F}

# looking at all possible interaction terms of variables chosen in modelback
lm(G3 ~(schoolMS + sexM + Mjobhealth + Mjobservices + Fjobteacher + studytime + failures + 
          schoolsupyes + famsupyes + paidyes + internetyes + goout + health + absences)^2, 
   data = student_d) %>%
  summary()
```

Of all possible interaction terms, several were significant. Among those, the most "interpretable" are studytime:schoolsupyes, studytime:famsupyes, studytime:goout, and schoolsupyes:goout. Interestingly, it appears that the effects of studytime are reduced when one goes out a lot; perhaps fatigue reduces the effectiveness of studying. 

```{r}
model.int <- lm(G3 ~ schoolMS + sexM + Mjobhealth + Mjobservices + Fjobteacher + studytime + 
                  failures + schoolsupyes + famsupyes + paidyes + internetyes + goout + health +
                  absences + studytime:schoolsupyes + studytime:famsupyes + studytime:goout + schoolsupyes:goout, 
   data = student_d)
```

### Small Model

```{r}
modelsmall <- lm(G3 ~ Mjobhealth + Mjobservices + Fjobteacher + failures + schoolsupyes + goout,
                 data = student_d)
```

#### Diagnostics of modelback

```{r}
stud_resid = rstudent(modelback)
dat_resid <- data.frame(RESID = stud_resid, ABS = abs(stud_resid), PRED = modelback$fitted.values)
dat_resid %>% arrange(-ABS) %>% filter(ABS > 2)
```

```{r}
nortest::ad.test(dat_resid$RESID)
```

```{r, fig.height=4, fig.width=8}
p1 <- ggplot(data = dat_resid) + geom_histogram(aes(RESID), bins = 25) + labs(x = "Studentized Residuals")
p2 <- ggplot(data = dat_resid) + geom_point(aes(PRED, RESID)) + geom_hline(aes(yintercept = 0)) + labs(x = "Predicted Values", y = "Studentized Residuals")
cowplot::plot_grid(p1, p2)
```

### Diagnostics of model.int

```{r}
stud_resid = rstudent(model.int)
dat_resid <- data.frame(RESID = stud_resid, ABS = abs(stud_resid), PRED = model.int$fitted.values)
dat_resid %>% arrange(-ABS) %>% filter(ABS > 2)
```

```{r}
nortest::ad.test(dat_resid$RESID)
```

Normality assumption is justified for model.int

```{r, fig.height=4, fig.width=8}
p1 <- ggplot(data = dat_resid) + geom_histogram(aes(RESID), bins = 25) + labs(x = "Studentized Residuals")
p2 <- ggplot(data = dat_resid) + geom_point(aes(PRED, RESID)) + geom_hline(aes(yintercept = 0)) + labs(x = "Predicted Values", y = "Studentized Residuals")
cowplot::plot_grid(p1, p2)
```

model.int satisfies linearity and homogeneity assumptions.

### Diagnostics of modelsmall

```{r}
stud_resid_small = rstudent(modelsmall)
dat_resid_small <- data.frame(RESID = stud_resid_small, ABS = abs(stud_resid_small), PRED = modelsmall$fitted.values)
dat_resid_small %>% arrange(-ABS) %>% filter(ABS > 2)
```

```{r}
nortest::ad.test(dat_resid_small$RESID)
```

Small model satisfies conditions for Gaussian assumption.

```{r, fig.height=4, fig.width=8}
p1.small <- ggplot(data = dat_resid_small) + geom_histogram(aes(RESID), bins = 25) + labs(x = "Studentized Residuals")
p2.small <- ggplot(data = dat_resid_small) + geom_point(aes(PRED, RESID)) + geom_hline(aes(yintercept = 0)) + labs(x = "Predicted Values", y = "Studentized Residuals")
cowplot::plot_grid(p1.small, p2.small)
```

The small model also appears to satisfy the diagnostic checks for homogeneity and linearity. Existence is trivial, but we may to discuss limitations of the independence assumption (maybe kids form study groups or grades are based on a curve)

## Extreme Residual Analysis  

Influence -- Cook's Distance
 - Values near 1 are bad 

Outlier -- Leverage Values 
 - Values greater than 2*p/n are bad

```{r}
p <- modelback$coefficients %>% length()
n <- nrow(student_d)
levcutoff <- 2*p/n
lev <- hatvalues(modelback)
cook <- cooks.distance(modelback)
```

take out values that are outlliers and run the regression again?

## Model Selection and Interpretation

Aside from looking at the Adjusted R2, can look at the AIC or SBC/BIC (smaller is better in both cases) **can add more/different models to this code, but here is the basis**

```{r}
## AIC Comparison
aic <- c(AIC(model1), AIC(model2), AIC(modelback), AIC(model.int), AIC(modelsmall))
bic <- c(BIC(model1), BIC(model2), BIC(modelback), BIC(model.int), BIC(modelsmall))
adjR <- c( summary(model1)$adj.r.sq,  summary(model2)$adj.r.sq,  summary(modelback)$adj.r.sq, summary(model.int)$adj.r.sq, summary(modelsmall)$adj.r.sq)

selection <- data.frame(model = c("Model1: Full Model", "Model2: No Intercept", "Model3: Stepwise Model", "Model 4: Stepwise Model + Interaction Termsg", "Model 5: Reduced Stepwise Model"), AICs = aic, BICs = bic, Adjusted.R.Squared = adjR)
selection %>% knitr::kable(align = c("c", "c"))
```

## Conclusion

## References 

[1] **Insert Dataset Reference** 

